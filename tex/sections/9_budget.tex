\chapter{Readout Infidelity Budget} \label{chap:budget}
In this chapter, we will use the realistic simulation to estimate the contributions to state initialization and measurement infidelities. We will also estimate the fidelity gains by improving the superconducting qubit and hardware through three physical parameters. 

\section{Turning off the Contributions}
In this section, we hypothesize three contributions to our SPAM fidelity. By  turning them on and off in the model, we will try to quantize the effect they have on the errors of our readout procedure. The three effects we will investigate are.
\begin{enumerate}
    \item A low readout efficiency, $\eta$, which results in an overlap between the $\ket{0}$ and $\ket{1}$ distributions in the IQ plane. This results in readout assignment error 
    \item The temperature, $\tau$, contributes significantly to the initialization error. At non-zero temperature, we will expect to see a mixed state with non-zero elements for $\ket{1}\bra{1}$. These errors will not be detected during readout. 
    \item Energy exchanges with the environment can give a low coherence time $T_1$. Since relaxation or excitation during the readout will alter the trajectory and can lead to false classifications.
\end{enumerate}
By using the simulation model, we can now turn off the different contributions and see which resulting fidelity the readout sequence achieves. As a start, we will model the perfect system. That is a system where the qubit has infinite lifetime $T_1 = \infty$, the system will be zero temperature, $\tau = 0$ and we detect all of the signal $\eta = 1$. A comparison of the Fidelity of this perfect system and the realistic one, which we also simulated in section \ref{sec:readout_in_simulation} can be seen in table \ref{tab:realistic_perfect_comparison} and the corresponding IQ plot is found in figure \ref{fig:realistic_perfect_comparison}. To see weights, histograms and other details, we refer to appendix \ref{chap:IQ_plots}.
\begin{figure}
    \centering
    \includegraphics[]{Simulations/budgets/figures/iq_scatter_budgetting_on_off_two.pdf}
    % \missingfigure[figheight = \textheight / 3]{IQ Plots for the simulation, we ran in the table}
    \caption{The IQ distributions for the experiment run with a realistic and an ideal set of parameters. The fidelity as well as separation line is shown on the plot. }
    \label{fig:realistic_perfect_comparison}
\end{figure}

\begin{margintable}[-2 cm]
    \centering
    \caption{Results from running the simulation experiment for 500 samples with a realistic set of parameters and a perfect set of parameters.}
    \begin{tabular}{l|r}
    \hline
         Parameters Set & Fidelity \\ \hline
         Realistic & 0.654 $\pm$ 0.024  \\
         Perfect   & 1.000 $\pm$ 0.000  
    \end{tabular}
    \label{tab:realistic_perfect_comparison}
\end{margintable}
In the experiment with  model with ideal parameters , we get fidelity of $F_{\text{SPAM}} = 1$. This supports our hypothesis where eliminating all thre parameters give a perfect classification . In the IQ plot  few points can be seen near origo. They are suspected to come from the effect, we described in section \ref{sec:hilbert_space} where  the SME in a finite dimensional Hilbert Space produces some unwanted dynamics when efficiency is high\footnote{The suspicion comes from seeing many more of these points with a lower dimensional Hilbert Space}. 


In an attempt to single out the effect on readout fidelity from the individual contributions, we can repeat the experiment with the different combinations of turning the effects on one after another. This leads to six experiments. Starting from the realistic set of parameters, we have three experiments with $\eta = 1$, $T_1 = \infty$, $\tau = 0$ respectively (each which excludes one source of SPAM errors). This is repeated with two out of three of the perfect parameters such that we exclusively have one source of SPAM errors. The IQ plots can be seen in figure \ref{fig:on_off_IQ_scatter_combinations} and the associated fidelity scores in table \ref{tab:readout_infidelity_contribution_estimation}.
\begin{figure}[h]
    \centering
    \includegraphics[]{Simulations/budgets/figures/iq_scatter_budgetting_on_off.pdf}
    % \missingfigure[figheight = \textheight / 3]{IQ Plots for the simulation, we ran in the table}
    \caption{The IQ results from turning on the different sources of errors one by one. The IQ scatter plots are displayed along side the decision boundary and the readout fidelity.}
    \label{fig:on_off_IQ_scatter_combinations}
\end{figure}
\begin{table}[h]
\centering
\caption{The Fidelity results of running the simulation experiment with 1000 samples given parameters that sets one or two of the three parameters to the ideal setting.}
\begin{tabular}{l|rrr}
\hline
\textbf{Contributions}        & Temperature         & Energy Decay   & Inefficiency\\ \hline
Excluding                     &  0.828 $\pm$ 0.018  &  0.702 $\pm$  0.022  &  0.746 $\pm$ 0.021\\
Exclusively                   &  0.734 $\pm$ 0.021  &  0.958 $\pm$  0.009  &  0.900 $\pm$ 0.014
\end{tabular}
\vspace{0.3 cm}
\label{tab:readout_infidelity_contribution_estimation}
\end{table}

Another way of visualizing the infidelity from each parameter is by placing them in a combination tree. We now add one contribution at a time according to the possible permutations and register the increase in infidelity. This will allow us to get a good idea of the contribution from the individual sources. The combination tree can be seen in figure \ref{fig:combination_tree_budget}. With infidelity errors of around $2-3\%$ (percent points) the differences have errors of $3-4 \%$ (percent points), so we should be careful making conclusions from the specific values. We can, however, analyse the overall pattern. 

At a first glance, the errors does not seem to be independent. Decay adds 7.2 \% infidelity if we have inefficient measurements while the infidelity goes down if we have non-zero temperature. While this could be a 2 sigma outlier, we should be careful when separating the contributions as it looks like there are some second order effects at play as well. If we want to get a vague first order estimate, we can take the simple average over infidelity contributions for the different parameters in the combination tree. These averages are shown in table \ref{tab:average_combination_tree}. 
\begin{margintable}[-4 cm]
    \centering
    \caption{Average contribution to infidelity when counting in the combination tree seen in figure \ref{fig:combination_tree_budget}}
    \vspace{0.3 cm}
    \begin{tabular}{l|r}
    \hline
    Parameter       &  Avg Infidelity\\ \hline
    Temperature     & 0.21 \\
    Decay           & 0.04 \\
    Efficiency      & 0.09 \\
    \end{tabular}
    \label{tab:average_combination_tree}
\end{margintable}
\begin{figure*}
    \centering
    \includegraphics{Figs/Results/combination_tree.png}
    \caption{Visualizing the different ways of combining the error sources and watching their contributions at each step.}
    \label{fig:combination_tree_budget}
\end{figure*}
For our system with a readout pulse defined in section $\ref{sec:overview_section}$, the $F_{\text{SPAM}}$ seems to be dominated by the temperature, followed by inefficiency and only slightly by the energy decay to the environment. %As we will discuss in the next section, there exists multiple tradeoffs which one can use a good performing parameters to reduce the effect from o trade a good parameter for performance corresponding to another.




\section{Improving the Readout}
In this section, we will setup a use case for the developed model: \textit{Given the device and the readout pulse parameters used throughout the thesis, what increase in $F_{\text{SPAM}}$ can we the expect by improving the physical device?}. We will use the model to estimate answers and use this as a basis to discuss how the parameters can be used to improve the performance in other parts of the readout sequence by applying other techniques. 



% In the previous section, we gave some estimates on how much three different sources contributed to the infidelity in the readout and initialization process. Although these numbers are interesting, an ideal qubit with infinite lifetime or a cooling system capable off sub mini kelvin temperatures are not realistic and is probably not what one should strive for. Instead, we will in this section present more marginal improvements like what would happen if the temperature is reduced by 10\% or 25\%. Furthermore, we will cover some strategies which can be implemented to deal with some of the errors in the straightforward 1 Âµs readout pulse which we have considered until now.

\subsection{Modifying the Parameters}
\begin{figure*}[t]
    \centering
    \includegraphics[width = \linewidth]{Simulations/budgets/figures/fidelities_at_different_parameters.pdf}
    \caption{The Fidelities of the model when the parameters are changed. In each plot the two other parameters are held constant at the realistic value.}
    \label{fig:scan_of_fidelities_at_different_parameters}
\end{figure*}
In the previous section, we considered the parameters to be on (at $0$) or off (at $100 \%$) of the considered value. We will now try more incremental improvements of one parameter while we keep the other two fixed. Again, we will run the simulation for 600 ns and do 500 trajectories for both initialization. All the fidelities of the readout sequence are summarized \ref{fig:scan_of_fidelities_at_different_parameters} and the IQ plots can be found in \ref{fig:budgetting_IQ_plots}. We will in the next three subsections discuss the results one parameter at a time. To see histogram distributions or weights, we refer to Appendix \ref{chap:IQ_plots}.

% In this section, we will instead create experiment with fractional improvements compared to the system parameters. To make the comparison somewhat fair, we will make sure to define quantities such that $0\%$ is perfect thus we will try different values for decay rate $\Gamma_1 = 1 / T_1$, inefficiency $1 - \eta$ and temeprature $\tau$. For each of these parameters, we run an experiment with the value at $0, 50\%, 75\%, 90\%, 100\%$ and $110\%$ of the initial value while keeping the other two parameters fixed. The results can be seen in figure \ref{fig:scan_of_fidelities_at_different_parameters} and in the tables: \ref{tab:temperature_contribution_estimation}, \ref{tab:decay_contribution_estimation} and \ref{tab:readout_infidelity_contribution_estimation}. The IQ plots of for the different experiments can be seen in, \ref{fig:budgetting_IQ_plots} and to see the weights and histogram used to calculate the fidelity refer to appendix \ref{appendix}... 


\subsection{Temperature}
In our model, the temperature has two roles: to determine the mixed state in the beginning of the readout and the relation between energy excitation and decays. Especially, the first of these two affects increases the state initialization error significantly as the failed initialization of $\ket{0}$ will be indistinguishable to the proper initialization of $\ket{1}$. In table \ref{tab:temperature_contribution_estimation}, we have summarized the SPAM-fidelities for experiments with a reduced temperature of $-10$, $0, 10, 25, 50$ and a $100$ percent. 

\begin{margintable}
\centering
\caption{The outcome of calibrating the qubit with the methods presented in this chapter for different temperatures.}
\vspace{0.3 cm}
\begin{tabular}{lr|c}
\hline
\textbf{Reduction}        & Temperature                  & Fidelity (\%)\\ \hline
-10 \%                    &  $162.3 $ mK       &  $58 \pm 3$\\
0   \%                    &  $147.5 $ mK       &  $65 \pm 2$\\
10  \%                    &  $132.8 $ mK       &  $69 \pm 2$\\
25  \%                    &  $110.6 $ mK       &  $75 \pm 2$\\
50  \%                    &  $\; 73.5$  mK     &  $82 \pm 2$\\
100 \%                    &  $\;\; 0.0$  mK    &  $83 \pm 2$\\
\end{tabular}
\label{tab:temperature_contribution_estimation}
\end{margintable}
For our system, the temperature decreases the Fidelity dramatically, since the equilibrium position will be around 88 percent $\ket{0}\bra{0}$ and 12 percent of $\ket{1}\bra{1}$. Since a reduction of temperature would exponentially suppress the excited part, a reduction of temperature by 50\% would reduce the state initialization error to mere $e^{-2}\approx 1/7$ of the current setup. This means that it should be highly prioritized to get to around this point, but further reduction  would lead to diminishing returns.

\paragraph{Active Reset} One way of artificially "cooling down" the qubit is by initializing with readout. If the readout fidelity is larger than the state preparation, it is beneficial to start with a readout and apply an X-gate if the qubit is in state $\ket{1}$. The qubit can the be read out and the X-gate reapplied until the outcome of the measurement is $\ket{0}$ (see figure \ref{fig:active_reset}.

To gain an advantage from active reset, we require the qubit to have long coherence time and high efficiency such that we are confident in our readout. The long coherence time is necessary since a normal readout would try to determine the state at the beginning of the pulse. An x-gate should however only be performed if the qubit is in state $\ket{1}$ so many repetitions would be required if the qubit decays in the meantime. In addition, a readout pulse leaves the resonator filled resulting in a shifted qubit frequency. If one does not take this into account when applying gates, the gate fidelity will be highly affected. The easiest approach is to wait for the resonator to return to the vacuum state, but this also requires the qubit to stay coherent during the process.  
\begin{marginfigure}[- 5 cm]
    \centering
    \includegraphics[]{Figs/circuits/active_reset.png}
    \caption{Illustration of the active reset process. A readout is performed, if it measures $\ket{1}$ an $X$-gate and another readout is applied. This is repeat till the readout gives $\ket{0}$.}
    \label{fig:active_reset}
\end{marginfigure}

\subsection{Qubit Decay}
Unwanted changes of the qubit between $\ket{0}\leftrightarrow\ket{1}$ is another big challenge when measuring the state of the qubit. If the qubit changes state during the readout process, it will be difficult to classify as either $\ket{0}$ or $\ket{1}$ and will often lead to points floating in between in the regular distributions in the IQ plots. In table \ref{tab:decay_contribution_estimation}, we see how small reductions in the transition rate increase the readout fidelity. 
\begin{margintable}
\centering
\caption{The outcome of calibrating the qubit with the methods presented in this chapter.}
\vspace{0.3 cm}
\begin{tabular}{lll|c}
\hline
\textbf{Reduction}         &  $\Gamma_1$                  &  $T_1$        & Fidelity (\%)\\ \hline
-10 \%                     &  $0.256 \text{ Âµs}^{-1}$         &  $3.91$ Âµs   &  $66 \pm 2$\\
0   \%                     &  $0.232 \text{ Âµs}^{-1}$         &  $4.30$ Âµs   &  $65 \pm 2$\\
10  \%                     &  $0.209 \text{ Âµs}^{-1}$         &  $4.78$ Âµs   &  $59 \pm 3$\\
25  \%                     &  $0.175 \text{ Âµs}^{-1}$         &  $5.73$ Âµs   &  $64 \pm 2$\\
50  \%                     &  $0.116 \text{ Âµs}^{-1}$         &  $8.60$ Âµs   &  $70 \pm 2$\\
100 \%                     &  $0$         &  $\infty$                       &  $70 \pm 2$\\
\end{tabular}
\label{tab:decay_contribution_estimation}
\end{margintable}
In section \ref{sec:postselection}, we saw that we can remove most of the measurement errors from this tail by repeating the experiment and only selecting the most certain measurements. However, this will severely impact the ability to run codes and will not be feasible in scenarios like error correcting codes, where a readout is performed on 10s or 100s of qubit in every cycle. 

With a pulse of $600 \text{ ns}$, the contribution from the energy decay during readout seems be less compared to that of efficiency and temperature. Thus, we would probably gain a lot of benefit using the active reset or increasing the pulse length a bit. If we want high-fidelity readout (> 99 \%), we should however find ways to decrease the infidelity contribution from $T_1$ either by improving the device or applying one of the following two adjustments.

\paragraph{Shorter Readout Pulse} One way of decreasing the effect of $T_1$ on readout fidelity by making a shorter readout pulse so the qubit has less time to decay. This will however require a higher efficiency since the distance between the two distributions should be at least 3-4 standard deviations to have sub-percent overlap. The width of the distribution scales with $1 / \sqrt{\eta t_{\text{readout}}}$, so a higher efficiency and a shorter pulse could lead to the same results.

\paragraph{Further Excitement} Another method for reducing the impact of qubit decay in readout is by applying a qubit pulse at frequency $f_{12}$ such that we drive transitions $\ket{1}\to\ket{2}$. The $\ket{0}$ will not be affected. And we can choose our readout frequency such that we can draw a decision boundary separating $\ket{0}$ or $\ket{\text{not }0}$ with high fidelity. Since the second excited state $\ket{2}$ primarily relaxes to $\ket{1}$, this will effectively lead to a higher lifetime since $\ket{2}$ and $\ket{1}$ will be classified similarly as $\ket{\text{not } 0}$. Unfortunately, this method can not be combined with active reset, since we only want to apply and x-gate if the qubit is in state $\ket{1}$ and not in this case, the indistinguishable $\ket{2}$. This could be solved with good qutrit classification \cite{chen_transmon_reaout}.

% For this reason the further excitement can not be combined with the active reset method presented in the previous subsection.

% Now the readout frequency can be altered to make sure that the Q-function for $\ket{1}$ and $\ket{2}$ are close and easily separable from $\ket{0}$. Since the second excited state $\ket{2}$ first will have to decay to $\ket{1}$ before decaying to $\ket{0}$ \todo{Footnote about fermis golden rule and how this supresses the direct transition} it will increase the readout fidelity. This however comes at the cost of not classifying $\ket{0}$ or $\ket{1}$, but will instead give us $\ket{0}$ or $\ket{\text{not }0}$ since the state could have been in either of the excited states in the measurement. For this reason the further excitement can not be combined with the active reset method presented in the previous subsection.

\subsection{Efficiency}
The efficiency plays a delicate role for reading out. Primarily, it scales the standard deviations of the two Gaussian distributions in the IQ plane with a factor of $\sqrt{\eta}$. The overlap between two Gaussians goes rapidly down with smaller widths, and with a separation of only three to four standard deviations, we are below the sub-percent overlap. From this points onward, we will not see much more performance gain. In table \ref{tab:readout_infidelity_contribution_estimation}, we see that a 4 time increase in $\eta$ leads to approximately the same performance gain as by having a perfect efficiency. If it is possible to increase the efficiency more, it will most likely be more beneficial to decrease the pulse duration. Instead of improving the efficiency directly, we can use the following two strategies to combat infidelity effects from inefficiency.

% Since it gives a rough estimate to get two Gaussian distributions in the IQ plot., the overlap scales with the amount of standard deviations they are apart. While having a distance at $< 3\sigma$ give an overlap of order a few percent, this drops rapidly after and when the distributions are $5 \sigma$ apart the overlap is negligible especially compared to the contributions from decay and temperature. To see the effect of reducing the inefficiency refer to table \ref{tab:readout_infidelity_contribution_estimation}.. 

\begin{margintable}
\centering
\caption{The outcome of calibrating the qubit with the methods presented in this chapter.}
\begin{tabular}{rr|c}
\hline
\textbf{Increase}       &  Efficiency       & Fidelity (\%)\\ \hline
-25 \%                  &  $\;2.64$ \%        &  $65 \pm 2$\\
0   \%                  &  $\;3.30$ \%        &  $65 \pm 2$\\
33  \%                  &  $\;4.40$ \%        &  $68 \pm 2$\\
100  \%                 &  $\;6.60$ \%        &  $68 \pm 2$\\
300  \%                 &  $13.2$ \%          &  $73 \pm 2$\\
max                     &  $100.0$ \%         &  $75 \pm 2$\\
\end{tabular}
\label{tab:readout_infidelity_contribution_estimation}
\end{margintable}

\paragraph{Longer Readout Pulse} The duration of a pulse is a compromise between energy decay and readout efficiency. If the pulse is too short the two distributions have a big overlap making it hard to classify them, whereas a long pulse increases the probability of energy decay. If we were to have a large $T_1$ and a low efficiency. In future setups, the optimal pulse length could be calibrated by doing an experiment where SPAM fidelity is determined as a function of time. 

\paragraph{High Power Readout - } The efficiency helps reduce the overlap of the two distribution by making the distributions narrower. However, another approach is to move the distributions further away from each other. In this thesis, we have been limited by the photon number for numerical reasons. However, by driving the resonator with a larger amplitude, the two distributions would move further away in the IQ-space. One should however be aware of the increased shift of the qubit frequency and to still be within the critical photon number where the dispersive approximation is valid or venture into a new realm of high power readouts. 


\begin{figure}
    \centering
    \includegraphics{Simulations/budgets/figures/iq_scatter_budgetting.pdf}
    \caption{The IQ Plots for the models  with different parameters. The seperation line, fidelity and parameter scaling is shown.}
    \label{fig:budgetting_IQ_plots}
\end{figure}
\FloatBarrier

\section{Further Path to Optimization}
In the sections above, we have considered a first order optimization, where all except one parameter were kept constant. This gives significant improvements for reductions in temperature, less for increased $\eta$ and marginal increases from better $T_1$. In addition to the improvements from a colder, more coherent and more efficient setup, we also discussed the possibility of "trading" a good performing parameter to improvements in the others. Some of the trade-offs are illustrated in figure \ref{fig:trading_performance}. 

\begin{marginfigure}
    \centering
    \includegraphics[]{Figs/trading_parameters.png}
    \caption{Illustration of how good coherence, low temperatures or efficient measurement can be used to reduce infidelity contribution from the other sources. }
    \label{fig:trading_performance}
\end{marginfigure}

The biggest factor in our SPAM errors was the temperature providing a mixed state from the beginning. Since the $T_1$ seemed to be less limiting, we would probably benefit from using the active reset method in our approach. Furthermore, we saw that the contribution from efficiency is larger than that of energy decay. This indicates that we could win performance by using a longer pulse. This consideration can be used in an experiment where an accumulated demodulation gives a fidelity for each pulse duration. One can now pick the duration with the highest fidelity.

Since all improvements would lead to an altered readout sequence, the improvement gained by reducing the parameters in the section above are not very representative of what would actually happen in the laboratory. To increase performance, a changed parameter would quickly start a search for the optimal readout duration, amplitude and applying the tricks (like active reset) that would help with our setup. In order to use the simulation model we have built up through this thesis, the next steps would be to find ways of simulating these strategies, so they can be applied when we do our forecasts. This will also lead to second order effects like inefficiency contributing to the state preparation error in active reset.

Another challenge is that a stronger pulse would definitely help our readout fidelity. The increased amplitude will force us to use a larger Hilbert space as well. And since the Lindblad Equation and the Stochastic Master Equation both evolve the density matrix, we would effectively get $n^2$ differential equation to solve. Implementing active reset in simulation is also not trivial. In the code-module built for this thesis, the simulation of a trajectory is separate from the analysis and classification from the trajectories. In active reset, we would have to do a simulation, readout analysis and apply a conditional x-gate  before simulating again\footnote{And might even have to repeat multiple times if the readout the second time still gives $\ket{1}$}. We would also need to take cool down of the resonator into account along with the possibility of qubit decay in the meantime.  Ultimately, this would mean a 3-10 times increase  (at best)  of simulation time and would require a rework of the simulations module.